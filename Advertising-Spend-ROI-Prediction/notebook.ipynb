{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowpark For Python -- Advertising Spend and ROI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from snowflake.snowpark.functions import (array_construct, call_udf, col, lit,\n",
    "                                          month, sum, udf, year)\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import (DateType, FloatType, IntegerType,\n",
    "                                      StringType, StructField, StructType,\n",
    "                                      Variant)\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Secure Connection to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create objects to use for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area PYTHON_CODE successfully created.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session.sql(\n",
    "#     \"\"\"CREATE WAREHOUSE IF NOT EXISTS SNOWPARK_DEMO_WH \n",
    "#            WITH WAREHOUSE_SIZE = 'MEDIUM' \n",
    "#                 WAREHOUSE_TYPE = 'STANDARD' \n",
    "#                 AUTO_SUSPEND = 60 \n",
    "#                 AUTO_RESUME = TRUE \n",
    "#                 INITIALLY_SUSPENDED = TRUE;\"\"\"\n",
    "# ).collect()\n",
    "session.sql(\"CREATE DATABASE IF NOT EXISTS SNOWPARK_ROI_DEMO;\").collect()\n",
    "session.sql(\"DROP SCHEMA IF EXISTS SNOWPARK_ROI_DEMO.PUBLIC;\").collect()\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS SNOWPARK_ROI_DEMO.AD_DATA;\").collect()\n",
    "session.sql(\n",
    "    \"\"\"CREATE FILE FORMAT IF NOT EXISTS SNOWPARK_ROI_DEMO.AD_DATA.CSVFORMAT \n",
    "           SKIP_HEADER = 1 \n",
    "           TYPE = 'CSV';\"\"\"\n",
    ").collect()\n",
    "session.sql(\n",
    "    \"\"\"CREATE STAGE IF NOT EXISTS SNOWPARK_ROI_DEMO.AD_DATA.CAMPAIGN_DATA_STAGE \n",
    "           FILE_FORMAT = SNOWPARK_ROI_DEMO.AD_DATA.CSVFORMAT  \n",
    "           URL = 's3://sfquickstarts/Summit 2022 Keynote Demo/campaign_spend/';\"\"\"\n",
    ").collect()\n",
    "session.sql(\n",
    "    \"\"\"CREATE STAGE IF NOT EXISTS SNOWPARK_ROI_DEMO.AD_DATA.MONTHLY_REVENUE_DATA_STAGE \n",
    "           FILE_FORMAT = SNOWPARK_ROI_DEMO.AD_DATA.CSVFORMAT  \n",
    "           URL = 's3://sfquickstarts/Summit 2022 Keynote Demo/monthly_revenue/';\"\"\"\n",
    ").collect()\n",
    "session.sql(\"CREATE OR REPLACE STAGE SNOWPARK_ROI_DEMO.AD_DATA.PYTHON_MODELS\").collect()\n",
    "session.sql(\"CREATE OR REPLACE STAGE SNOWPARK_ROI_DEMO.AD_DATA.PYTHON_CODE\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.use_warehouse(\"SNOWPARK_DEMO_WH\")\n",
    "session.use_database(\"SNOWPARK_ROI_DEMO\")\n",
    "session.use_schema(\"AD_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : RFAJRI27\n",
      "Role                        : ACCOUNTADMIN\n",
      "Database                    : SNOWPARK_ROI_DEMO\n",
      "Schema                      : AD_DATA\n",
      "Warehouse                   : COMPUTE_WH\n",
      "Snowflake version           : 7.30.0\n",
      "Snowpark for Python version : 1.7.0\n"
     ]
    }
   ],
   "source": [
    "snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(snowflake_environment[0][1]))\n",
    "print('Database                    : {}'.format(snowflake_environment[0][2]))\n",
    "print('Schema                      : {}'.format(snowflake_environment[0][3]))\n",
    "print('Warehouse                   : {}'.format(snowflake_environment[0][5]))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][4]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the files in the external stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='s3://sfquickstarts/Summit 2022 Keynote Demo/campaign_spend/campaign_spend.csv', size=13684943, md5='1d87f70421662a7666d3918b16b81daa', last_modified='Fri, 5 Aug 2022 20:22:18 GMT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"LS @CAMPAIGN_DATA_STAGE\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Aggregated Campaign Spend Data from Snowflake table into Snowpark DataFrame\n",
    "Let's first load the campaign spend data. This table contains ad click data that has been aggregated to show daily spend across digital ad channels including search engines, social media, email and video.\n",
    "\n",
    "NOTE: Ways to load data in a Snowpark Dataframe\n",
    "\n",
    "* session.table(\"db.schema.table\")\n",
    "* session.sql(\"select col1, col2... from tableName\")\n",
    "* session.read.parquet(\"@stageName/path/to/file\")\n",
    "* session.create_dataframe([1,2,3], schema=[\"col1\"])\n",
    "\n",
    "TIP: For more information on Snowpark DataFrames, refer to the [docs](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/_autosummary/snowflake.snowpark.html#snowflake.snowpark.DataFrame).\n",
    "\n",
    "Query and preview the CSV file in the stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "|\"CAMPAIGN\"              |\"CHANNEL\"      |\"DATE\"      |\"TOTAL_CLICKS\"  |\"TOTAL_COST\"  |\"ADS_SERVED\"  |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "|winter_sports           |video          |2012-06-03  |213             |1762          |426           |\n",
      "|sports_across_cultures  |video          |2012-06-02  |87              |678           |157           |\n",
      "|building_community      |search_engine  |2012-06-03  |66              |471           |134           |\n",
      "|world_series            |social_media   |2017-12-28  |72              |591           |149           |\n",
      "|winter_sports           |email          |2018-02-09  |252             |1841          |473           |\n",
      "|spring_break            |video          |2017-11-14  |162             |1155          |304           |\n",
      "|nba_finals              |email          |2017-11-22  |68              |480           |134           |\n",
      "|winter_sports           |social_media   |2018-03-10  |227             |1797          |454           |\n",
      "|spring_break            |search_engine  |2017-08-30  |150             |1226          |302           |\n",
      "|uefa                    |video          |2017-09-30  |81              |701           |168           |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.sql(\n",
    "    \"\"\"SELECT $1::VARCHAR(60) AS CAMPAIGN, \n",
    "              $2::VARCHAR(60) AS CHANNEL, \n",
    "              $3::DATE AS DATE, \n",
    "              $4::NUMBER(38, 0) AS TOTAL_CLICKS, \n",
    "              $5::NUMBER(38, 0) AS TOTAL_COST, \n",
    "              $6::NUMBER(38, 0) AS ADS_SERVED \n",
    "       FROM @CAMPAIGN_DATA_STAGE\"\"\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write this DataFrame to a Snowflake table named `CAMPAIGN_SPEND`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save_as_table(\"CAMPAIGN_SPEND\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': ['SELECT  *  FROM (campaign_spend)'], 'post_actions': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_df_spend = session.table('campaign_spend')\n",
    "snow_df_spend.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "|\"CAMPAIGN\"              |\"CHANNEL\"      |\"DATE\"      |\"TOTAL_CLICKS\"  |\"TOTAL_COST\"  |\"ADS_SERVED\"  |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "|winter_sports           |video          |2012-06-03  |213             |1762          |426           |\n",
      "|sports_across_cultures  |video          |2012-06-02  |87              |678           |157           |\n",
      "|building_community      |search_engine  |2012-06-03  |66              |471           |134           |\n",
      "|world_series            |social_media   |2017-12-28  |72              |591           |149           |\n",
      "|winter_sports           |email          |2018-02-09  |252             |1841          |473           |\n",
      "|spring_break            |video          |2017-11-14  |162             |1155          |304           |\n",
      "|nba_finals              |email          |2017-11-22  |68              |480           |134           |\n",
      "|winter_sports           |social_media   |2018-03-10  |227             |1797          |454           |\n",
      "|spring_break            |search_engine  |2017-08-30  |150             |1226          |302           |\n",
      "|uefa                    |video          |2017-09-30  |81              |701           |168           |\n",
      "|uefa                    |video          |2018-01-23  |73              |545           |141           |\n",
      "|sports_across_cultures  |search_engine  |2017-10-12  |73              |544           |143           |\n",
      "|winter_sports           |social_media   |2018-01-14  |207             |1640          |418           |\n",
      "|youth_on_course         |search_engine  |2018-03-29  |164             |1036          |291           |\n",
      "|memorial_day            |social_media   |2018-01-18  |131             |1119          |281           |\n",
      "|family_history          |video          |2018-03-24  |88              |646           |166           |\n",
      "|memorial_day            |search_engine  |2017-12-09  |134             |968           |262           |\n",
      "|youth_in_action         |search_engine  |2018-03-24  |194             |1642          |411           |\n",
      "|winter_sports           |video          |2017-10-25  |208             |1673          |431           |\n",
      "|thanksgiving_football   |video          |2017-11-10  |82              |633           |165           |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QueryRecord(query_id='01aeb816-0000-0f1b-0000-000013319a25', sql_text='SELECT  *  FROM campaign_spend LIMIT 20')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action sends the DF SQL for execution\n",
    "# Note: history object provides the query ID which can be helpful for debugging as well as the SQL query executed on the server\n",
    "with session.query_history() as history:\n",
    "    snow_df_spend.show(20)\n",
    "history.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Spend per Channel per Month\n",
    "Let's transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"CHANNEL\"      |\"TOTAL_COST\"  |\n",
      "---------------------------------------------------\n",
      "|2012    |5        |search_engine  |516431        |\n",
      "|2012    |5        |video          |516729        |\n",
      "|2012    |5        |email          |517208        |\n",
      "|2012    |5        |social_media   |517618        |\n",
      "|2012    |6        |video          |501098        |\n",
      "|2012    |6        |search_engine  |506497        |\n",
      "|2012    |6        |social_media   |504679        |\n",
      "|2012    |6        |email          |501947        |\n",
      "|2012    |7        |search_engine  |522780        |\n",
      "|2012    |7        |email          |518405        |\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stats per Month per Channel\n",
    "snow_df_spend_per_channel = snow_df_spend.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n",
    "    with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n",
    "\n",
    "snow_df_spend_per_channel.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot on Channel\n",
    "Let's further transform the campaign spend data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions. This transformation will enable us to join with the revenue table such that we will have our input features and target variable in a single table for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\n",
      "---------------------------------------------------------------------------\n",
      "|2012    |5        |516431           |517618          |516729   |517208   |\n",
      "|2012    |6        |506497           |504679          |501098   |501947   |\n",
      "|2012    |7        |522780           |521395          |522762   |518405   |\n",
      "|2012    |8        |519959           |520537          |520685   |521584   |\n",
      "|2012    |9        |507211           |507404          |511364   |507363   |\n",
      "|2012    |10       |518942           |520863          |522768   |519950   |\n",
      "|2012    |11       |505715           |505221          |505292   |503748   |\n",
      "|2012    |12       |520148           |520711          |521427   |520724   |\n",
      "|2013    |1        |522151           |518635          |520583   |521167   |\n",
      "|2013    |2        |467736           |474679          |469856   |469784   |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_per_month = snow_df_spend_per_channel.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n",
    "snow_df_spend_per_month = snow_df_spend_per_month.select(\n",
    "    col(\"YEAR\"),\n",
    "    col(\"MONTH\"),\n",
    "    col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n",
    "    col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n",
    "    col(\"'video'\").as_(\"VIDEO\"),\n",
    "    col(\"'email'\").as_(\"EMAIL\")\n",
    ")\n",
    "snow_df_spend_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Revenue per Month\n",
    "Now let's load revenue table and transform the data into revenue per year/month using group_by and agg() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.sql(\n",
    "    \"\"\"SELECT $1::NUMBER(38, 0) AS YEAR, \n",
    "              $2::NUMBER(38, 0) AS MONTH, \n",
    "              $3::FLOAT AS REVENUE\n",
    "       FROM @MONTHLY_REVENUE_DATA_STAGE\"\"\")\n",
    "\n",
    "df.write.save_as_table(\"MONTHLY_REVENUE\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"REVENUE\"   |\n",
      "---------------------------------\n",
      "|2012    |5        |3264300.11  |\n",
      "|2012    |6        |3208482.33  |\n",
      "|2012    |7        |3311966.98  |\n",
      "|2012    |8        |3311752.81  |\n",
      "|2012    |9        |3208563.06  |\n",
      "|2012    |10       |3334028.46  |\n",
      "|2012    |11       |3185894.64  |\n",
      "|2012    |12       |3334570.96  |\n",
      "|2013    |1        |3316455.44  |\n",
      "|2013    |2        |2995042.21  |\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_revenue = session.table('monthly_revenue')\n",
    "snow_df_revenue_per_month = snow_df_revenue.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n",
    "snow_df_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Total Spend and Total Revenue per Month\n",
    "Next let's **join this revenue data with the transformed campaign spend data** so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "----------------------------------------------------------------------------------------\n",
      "|2012    |5        |516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|2012    |6        |506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|2012    |7        |522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|2012    |8        |519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|2012    |9        |507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|2012    |10       |518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|2012    |11       |505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|2012    |12       |520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|2013    |1        |522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|2013    |2        |467736           |474679          |469856   |469784   |2995042.21  |\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_and_revenue_per_month = snow_df_spend_per_month.join(snow_df_revenue_per_month, [\"YEAR\",\"MONTH\"])\n",
    "snow_df_spend_and_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>> Examine Snowpark DataFrame Query and Execution Plan <<<\n",
    "Snowpark makes is really convenient to look at the DataFrame query and execution plan using explain() Snowpark DataFrame function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------DATAFRAME EXECUTION PLAN----------\n",
      "Query List:\n",
      "1.\n",
      "SELECT  *  FROM (( SELECT \"YEAR\" AS \"YEAR\", \"MONTH\" AS \"MONTH\", \"SEARCH_ENGINE\" AS \"SEARCH_ENGINE\", \"SOCIAL_MEDIA\" AS \"SOCIAL_MEDIA\", \"VIDEO\" AS \"VIDEO\", \"EMAIL\" AS \"EMAIL\" FROM ( SELECT \"YEAR\", \"MONTH\", \"'search_engine'\" AS \"SEARCH_ENGINE\", \"'social_media'\" AS \"SOCIAL_MEDIA\", \"'video'\" AS \"VIDEO\", \"'email'\" AS \"EMAIL\" FROM ( SELECT  *  FROM ( SELECT  *  FROM ( SELECT \"YEAR(DATE)\" AS \"YEAR\", \"MONTH(DATE)\" AS \"MONTH\", \"CHANNEL\", \"TOTAL_COST\" FROM ( SELECT year(\"DATE\") AS \"YEAR(DATE)\", month(\"DATE\") AS \"MONTH(DATE)\", \"CHANNEL\", sum(\"TOTAL_COST\") AS \"TOTAL_COST\" FROM ( SELECT  *  FROM campaign_spend) GROUP BY year(\"DATE\"), month(\"DATE\"), \"CHANNEL\")) ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST) PIVOT (sum(\"TOTAL_COST\") FOR \"CHANNEL\" IN ('search_engine', 'social_media', 'video', 'email'))) ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST)) AS SNOWPARK_LEFT INNER JOIN ( SELECT \"YEAR\" AS \"YEAR\", \"MONTH\" AS \"MONTH\", \"REVENUE\" AS \"REVENUE\" FROM ( SELECT \"YEAR\", \"MONTH\", \"SUM(REVENUE)\" AS \"REVENUE\" FROM ( SELECT \"YEAR\", \"MONTH\", sum(\"REVENUE\") AS \"SUM(REVENUE)\" FROM ( SELECT  *  FROM monthly_revenue) GROUP BY \"YEAR\", \"MONTH\") ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST)) AS SNOWPARK_RIGHT USING (YEAR, MONTH))\n",
      "Logical Execution Plan:\n",
      "GlobalStats:\n",
      "    partitionsTotal=2\n",
      "    partitionsAssigned=2\n",
      "    bytesAssigned=1731072\n",
      "Operations:\n",
      "1:0     ->Result  EXTRACT(year from CAMPAIGN_SPEND.DATE), EXTRACT(month from CAMPAIGN_SPEND.DATE), 'search_engine', 'social_media', 'video', 'email', SUM(MONTHLY_REVENUE.REVENUE)  \n",
      "1:1          ->InnerJoin  joinKey: (EXTRACT(month from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.MONTH) AND (EXTRACT(year from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.YEAR)  \n",
      "1:2               ->Sort  EXTRACT(year from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST, EXTRACT(month from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST  \n",
      "1:3                    ->Pivot  'search_engine', 'social_media', 'video', 'email'  \n",
      "1:4                         ->Sort  EXTRACT(year from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST, EXTRACT(month from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST  \n",
      "1:5                              ->Aggregate  aggExprs: [SUM(CAMPAIGN_SPEND.TOTAL_COST)], groupKeys: [EXTRACT(year from CAMPAIGN_SPEND.DATE), EXTRACT(month from CAMPAIGN_SPEND.DATE), CAMPAIGN_SPEND.CHANNEL]  \n",
      "1:6                                   ->TableScan  SNOWPARK_ROI_DEMO.AD_DATA.CAMPAIGN_SPEND  CHANNEL, DATE, TOTAL_COST  {partitionsTotal=1, partitionsAssigned=1, bytesAssigned=1728512}\n",
      "1:7               ->Sort  MONTHLY_REVENUE.YEAR ASC NULLS FIRST, MONTHLY_REVENUE.MONTH ASC NULLS FIRST  \n",
      "1:8                    ->Aggregate  aggExprs: [SUM(MONTHLY_REVENUE.REVENUE)], groupKeys: [MONTHLY_REVENUE.YEAR, MONTHLY_REVENUE.MONTH]  \n",
      "1:9                         ->JoinFilter  joinKey: (EXTRACT(month from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.MONTH) AND (EXTRACT(year from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.YEAR)  \n",
      "1:10                              ->TableScan  SNOWPARK_ROI_DEMO.AD_DATA.MONTHLY_REVENUE  YEAR, MONTH, REVENUE  {partitionsTotal=1, partitionsAssigned=1, bytesAssigned=2560}\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_and_revenue_per_month.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training in Snowflake\n",
    "Features and Target\n",
    "At this point we are ready to perform the following actions to save features and target for model training.\n",
    "\n",
    "Delete rows with missing values\n",
    "Exclude columns we don't need for modeling\n",
    "Save features into a Snowflake table called MARKETING_BUDGETS_FEATURES\n",
    "TIP: To see how to handle missing values in Snowpark Python, refer to this [blog](https://medium.com/snowflake/handling-missing-values-with-snowpark-for-python-part-1-4af4285d24e6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "---------------------------------------------------------------------\n",
      "|516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|467736           |474679          |469856   |469784   |2995042.21  |\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete rows with missing values\n",
    "snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.dropna()\n",
    "\n",
    "# Exclude columns we don't need for modeling\n",
    "snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.drop(['YEAR','MONTH'])\n",
    "\n",
    "# Save features into a Snowflake table call MARKETING_BUDGETS_FEATURES\n",
    "snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGETS_FEATURES')\n",
    "snow_df_spend_and_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python function to train a Linear Regression model using scikit-learn\n",
    "Let's create a Python function that uses scikit-learn and other packages which are already included in Snowflake Anaconda channel and therefore available on the server-side when executing the Python function as a Stored Procedure running in Snowflake.\n",
    "\n",
    "This function takes the following as parameters:\n",
    "* session: Snowflake Session object.\n",
    "* features_table: Name of the table that holds the features and target variable.\n",
    "* number_of_folds: Number of cross validation folds used in GridSearchCV.\n",
    "* polynomial_features_degress: PolynomialFeatures as a preprocessing step.\n",
    "* train_accuracy_threshold: Accuracy thresholds for train dataset. This values is used to determine if the model should be saved.\n",
    "* test_accuracy_threshold: Accuracy thresholds for test dataset. This values is used to determine if the model should be saved.\n",
    "* save_model: Boolean that determines if the model should be saved provided the accuracy thresholds are met.\n",
    "\n",
    "TIP: For large datasets, Snowflake offers [Snowpark-optimized Warehouses](https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized.html) which are in Public Preview as of Nov 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_revenue_prediction_model(\n",
    "    session: Session, \n",
    "    features_table: str, \n",
    "    number_of_folds: int, \n",
    "    polynomial_features_degrees: int, \n",
    "    train_accuracy_threshold: float, \n",
    "    test_accuracy_threshold: float, \n",
    "    save_model: bool) -> Variant:\n",
    "    \n",
    "    import os\n",
    "\n",
    "    from joblib import dump\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "    # Load features\n",
    "    df = session.table(features_table).to_pandas()\n",
    "\n",
    "    # Preprocess the Numeric columns\n",
    "    # We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns\n",
    "    # NOTE: High degrees can cause overfitting.\n",
    "    numeric_features = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "    numeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree = polynomial_features_degrees)),('scaler', StandardScaler())])\n",
    "\n",
    "    # Combine the preprocessed step together using the Column Transformer module\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "    # The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\n",
    "    parameteres = {}\n",
    "\n",
    "    X = df.drop('REVENUE', axis = 1)\n",
    "    y = df['REVENUE']\n",
    "\n",
    "    # Split dataset into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "    # Use GridSearch to find the best fitting model based on number_of_folds folds\n",
    "    model = GridSearchCV(pipeline, param_grid=parameteres, cv=number_of_folds)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2_score = model.score(X_train, y_train)\n",
    "    test_r2_score = model.score(X_test, y_test)\n",
    "\n",
    "    model_saved = False\n",
    "    pipeline\n",
    "\n",
    "    if save_model:\n",
    "        if train_r2_score >= train_accuracy_threshold and test_r2_score >= test_accuracy_threshold:\n",
    "            # Upload trained model to a stage\n",
    "            model_output_dir = '/tmp'\n",
    "            model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    "            dump(model, model_file)\n",
    "            session.file.put(model_file,\"@PYTHON_MODELS\",overwrite=True)\n",
    "            model_saved = True\n",
    "\n",
    "    # Return model R2 score on train and test data\n",
    "    return {\"R2 score on Train\": train_r2_score,\n",
    "            \"R2 threshold on Train\": train_accuracy_threshold,\n",
    "            \"R2 score on Test\": test_r2_score,\n",
    "            \"R2 threshold on Test\": test_accuracy_threshold,\n",
    "            \"Model saved\": model_saved}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Python function before deploying it as a Stored Procedure on Snowflake\n",
    "Since we're in test mode, we will set save_model = False so that the model is not saved just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R2 score on Train': 0.9954552822793987,\n",
       " 'R2 threshold on Train': 0.85,\n",
       " 'R2 score on Test': 0.8817971097765084,\n",
       " 'R2 threshold on Test': 0.85,\n",
       " 'Model saved': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validaton_folds = 10\n",
    "polynomial_features_degrees = 2\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = False\n",
    "\n",
    "train_revenue_prediction_model(\n",
    "    session,\n",
    "    \"MARKETING_BUDGETS_FEATURES\",\n",
    "    cross_validaton_folds,\n",
    "    polynomial_features_degrees,\n",
    "    train_accuracy_threshold,\n",
    "    test_accuracy_threshold,\n",
    "    save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stored Procedure to deploy model training code on Snowflake\n",
    "Assuming the testing is complete and we're satisfied with the model, let's register the model training Python function as a Snowpark Python Stored Procedure by supplying the packages (snowflake-snowpark-python,scikit-learn, and joblib) it will need and use during execution.\n",
    "\n",
    "TIP: For more information on Snowpark Python Stored Procedures, refer to the [docs](https://docs.snowflake.com/developer-guide/stored-procedure/stored-procedures-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x7fb4989458e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=train_revenue_prediction_model,\n",
    "    name=\"train_revenue_prediction_model\",\n",
    "    packages=['snowflake-snowpark-python','scikit-learn==1.1.1','joblib'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@PYTHON_CODE\",\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Examine Query History in Snowsight <<<\n",
    "### Execute Stored Procedure to train model and deploy it on Snowflake\n",
    "Now we're ready to train the model and save it onto a Snowflake stage so let's set save_model = True and run/execute the Stored Procedure using session.call() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Model saved\": true,\n",
      "  \"R2 score on Test\": 0.8817971097765288,\n",
      "  \"R2 score on Train\": 0.9954552822793986,\n",
      "  \"R2 threshold on Test\": 0.85,\n",
      "  \"R2 threshold on Train\": 0.85\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cross_validaton_folds = 10\n",
    "polynomial_features_degrees = 2\n",
    "train_accuracy_threshold = 0.85\n",
    "test_accuracy_threshold = 0.85\n",
    "save_model = True\n",
    "\n",
    "print(session.call('train_revenue_prediction_model',\n",
    "                    'MARKETING_BUDGETS_FEATURES',\n",
    "                    cross_validaton_folds,\n",
    "                    polynomial_features_degrees,\n",
    "                    train_accuracy_threshold,\n",
    "                    test_accuracy_threshold,\n",
    "                    save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Examine Query History in Snowsight <<<\n",
    "### Create Scalar User-Defined Function (UDF) for inference\n",
    "Now to deploy this model for inference, let's create and register a Snowpark Python UDF and add the trained model as a dependency. Once registered, getting new predictions is as simple as calling the function by passing in data.\n",
    "\n",
    "NOTE: Scalar UDFs operate on a single row / set of data points and are great for online inference in real-time. And this UDF is called from the Streamlit App. See Snowpark_Streamlit_Revenue_Prediction.py\n",
    "\n",
    "TIP: For more information on Snowpark Python User-Defined Functions, refer to the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "# Add trained model and Python packages from Snowflake Anaconda channel available on the server-side as UDF dependencies\n",
    "session.add_import('@PYTHON_MODELS/model.joblib.gz')\n",
    "session.add_packages('pandas','joblib','scikit-learn==1.1.1')\n",
    "\n",
    "@udf(name='predict_roi',session=session,replace=True,is_permanent=True,stage_location='@PYTHON_CODE')\n",
    "def predict_roi(budget_allocations: list) -> float:\n",
    "    import sys\n",
    "\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    from joblib import load\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    \n",
    "    model_file = import_dir + 'model.joblib.gz'\n",
    "    model = load(model_file)\n",
    "            \n",
    "    features = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "    df = pd.DataFrame([budget_allocations], columns=features)\n",
    "    roi = abs(model.predict(df)[0])\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Scalar User-Defined Function (UDF) for inference on new data\n",
    "Once the UDF is registered, getting new predictions is as simple as calling the call_udf() Snowpark Python function and passing in new datapoints.\n",
    "\n",
    "Let's create a SnowPark DataFrame with some sample data and call the UDF to get new predictions.\n",
    "\n",
    "NOTE: This UDF is also called from the Streamlit App. See Snowpark_Streamlit_Revenue_Prediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"PREDICTED_ROI\"     |\n",
      "-----------------------------------------------------------------------------\n",
      "|250000           |250000          |200000   |450000   |4072491.441724832   |\n",
      "|500000           |500000          |500000   |500000   |3179613.166194174   |\n",
      "|8500             |9500            |2000     |500      |189866.83304576762  |\n",
      "-----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = session.create_dataframe([[250000,250000,200000,450000],[500000,500000,500000,500000],[8500,9500,2000,500]], \n",
    "                                    schema=['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL'])\n",
    "test_df.select(\n",
    "    'SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL', \n",
    "    call_udf(\"predict_roi\", \n",
    "    array_construct(col(\"SEARCH_ENGINE\"), col(\"SOCIAL_MEDIA\"), col(\"VIDEO\"), col(\"EMAIL\"))).as_(\"PREDICTED_ROI\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Snowpark Stored Procedures vs User-Defined Functions**\n",
    "\n",
    "In general, if you're processing a large dataset in a way where each row/batch can be processed independently - UDFs are always better, because the processing is automatically parallelized/scaled across the warehouse. For example, if you already have a trained ML model, and you're doing inference using that model on billions of rows. In that case, each row/batch can be computed independently.\n",
    "\n",
    "If the use case requires the full dataset to be in-memory (e.g. ML training), then a stored procedure is the way to go. A stored procedure is just a Python program that runs on a single warehouse node. (With a UDF it's not possible to load the full dataset into memory because the processing is done in a streaming fashion, one batch at a time.\n",
    "\n",
    "### Automate Data Pipeline and Model (re)Training using Snowflake Tasks\n",
    "We can also optionally create Snowflake (Serverless or User-managed) Tasks to automate data pipelining and (re)training of the model on a set schedule.\n",
    "\n",
    "NOTE: Creating tasks using Snowpark Python API (instead of SQL) is on the roadmap. Stay tuned! Or, follow me on [Twitter]((https://twitter.com/iamontheinet)) to get the news before anyone else :)_\n",
    "\n",
    "TIP: Amongst other things, you can also configure tasks for error notification (currently in Private Preview) using cloud messaging service such as Amazon Simple Notification Service (SNS). For more information on Snowflake Tasks, refer to the docs.\n",
    "\n",
    "#### Create Python Function for Data Pipeline and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline_feature_engineering(session: Session) -> str:\n",
    "\n",
    "  # DATA TRANSFORMATIONS\n",
    "  # Perform the following actions to transform the data\n",
    "\n",
    "  # Load the campaign spend data\n",
    "  snow_df_spend = session.table('campaign_spend')\n",
    "\n",
    "  # Transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions\n",
    "  snow_df_spend_per_channel = snow_df_spend.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n",
    "      with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n",
    "\n",
    "  # Transform the data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions\n",
    "  snow_df_spend_per_month = snow_df_spend_per_channel.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n",
    "  snow_df_spend_per_month = snow_df_spend_per_month.select(\n",
    "      col(\"YEAR\"),\n",
    "      col(\"MONTH\"),\n",
    "      col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n",
    "      col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n",
    "      col(\"'video'\").as_(\"VIDEO\"),\n",
    "      col(\"'email'\").as_(\"EMAIL\")\n",
    "  )\n",
    "\n",
    "  # Load revenue table and transform the data into revenue per year/month using group_by and agg() functions\n",
    "  snow_df_revenue = session.table('monthly_revenue')\n",
    "  snow_df_revenue_per_month = snow_df_revenue.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n",
    "\n",
    "  # Join revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training\n",
    "  snow_df_spend_and_revenue_per_month = snow_df_spend_per_month.join(snow_df_revenue_per_month, [\"YEAR\",\"MONTH\"])\n",
    "\n",
    "  # SAVE FEATURES And TARGET\n",
    "  # Perform the following actions to save features and target for model training\n",
    "\n",
    "  # Delete rows with missing values\n",
    "  snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.dropna()\n",
    "\n",
    "  # Exclude columns we don't need for modeling\n",
    "  snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.drop(['YEAR','MONTH'])\n",
    "\n",
    "  # Save features into a Snowflake table call MARKETING_BUDGETS_FEATURES\n",
    "  snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGETS_FEATURES')\n",
    "\n",
    "  return \"SUCCESS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Stored Procedure to deploy data pipelining feature engineeering code on Snowflake\n",
    "TIP: For more information on Snowpark Python Stored Procedures, refer to the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x7fb4992f8a90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(\n",
    "    func=data_pipeline_feature_engineering,\n",
    "    name=\"data_pipeline_feature_engineering\",\n",
    "    packages=['snowflake-snowpark-python'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@PYTHON_CODE\",\n",
    "    replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Stored Procedure to deploy data pipelining feature engineeering code on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "print(session.call('data_pipeline_feature_engineering'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Root/Parent Snowflake Task: Data pipelining and feature engineeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Task DATA_PIPELINE_FEATURE_ENGINEERING_TASK successfully created.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_data_pipeline_feature_engineering_task = \"\"\"\n",
    "CREATE OR REPLACE TASK data_pipeline_feature_engineering_task\n",
    "    WAREHOUSE = 'COMPUTE_WH'\n",
    "    SCHEDULE  = '1 MINUTE'\n",
    "AS\n",
    "    CALL data_pipeline_feature_engineering()\n",
    "\"\"\"\n",
    "session.sql(create_data_pipeline_feature_engineering_task).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Child/Dependent Snowflake Task: Model training on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Task MODEL_TRAINING_TASK successfully created.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_training_task = \"\"\"\n",
    "CREATE OR REPLACE TASK model_training_task\n",
    "    WAREHOUSE = 'COMPUTE_WH'\n",
    "    AFTER data_pipeline_feature_engineering_task\n",
    "AS\n",
    "    CALL train_revenue_prediction_model('MARKETING_BUDGETS_FEATURES',10,2,0.85,0.85,True)\n",
    "\"\"\"\n",
    "session.sql(create_model_training_task).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"alter task model_training_task resume\").collect()\n",
    "session.sql(\"alter task data_pipeline_feature_engineering_task resume\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"alter task data_pipeline_feature_engineering_task suspend\").collect()\n",
    "session.sql(\"alter task model_training_task suspend\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|\"A\"  |\"B\"  |\"C\"  |\"D\"  |\n",
      "-------------------------\n",
      "|1    |2    |3    |4    |\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = session.create_dataframe([[1, 2, 3, 4]], schema=[\"a\", \"b\", \"c\", \"d\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
